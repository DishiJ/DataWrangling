{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 2\n",
    "#### Student Name: DISHI JAIN\n",
    "#### Student ID: 30759307\n",
    "\n",
    "\n",
    "Libraries used:\n",
    "* import pandas as pd (for accessing DataFrames)\n",
    "* import ast (for ast.literal_eval)\n",
    "* import nltk (for sentiment intensity analyer)\n",
    "* import nltk.sentiment.vader as va (for sentiment intensity analyer)\n",
    "* import numpy as np (for np.where)\n",
    "* nltk.download('vader_lexicon') (for sentiment intensity analyer)\n",
    "\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "This assignment is focused on data cleansing and finding outliers in the data. Here the data represents orders places in Australia in different stores. We need to clean the data and fix any errors that it may have. Also we need to find the missing data and impute it from any other attribute that we can. We also need to detect and remove outliers from the data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import ast\n",
    "import nltk\n",
    "import nltk.sentiment.vader as va\n",
    "import numpy as np\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysing Dirty Data\n",
    "\n",
    "In this task we have to find errors in the file and clean or fix those errors. These errors can be in any form example in date column, diatancce column, etc. We need to find and fix such errors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading dirty_data csv file\n",
    "dirty_data = pd.read_csv(\"30759307_dirty_data.csv\")\n",
    "outlier_data = pd.read_csv(\"30759307_outlier_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the shape of the data and displaying its few top records\n",
    "print (dirty_data.shape) \n",
    "dirty_data.head(10)\n",
    "#get order of column\n",
    "cols = dirty_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting information about the data in terms of data type of columns and count of values in the column\n",
    "dirty_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describing the numerical data to get the count,min,max and some other attributes\n",
    "dirty_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above .describe() we can see that all the numerical columns have 500 entries. Hence there are no missing entries present. We can also see that the maximum value in the column distance_to_nearest_warehouse is approximately 5 which is very far away from the mean. So this could be an error. Also looking at the customer_lat and customer_long columns we can see that there exists and error as for this location the latitdes are negative and longitudes are positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describing the caegorical data to get the count,freq,unique and some other attributes\n",
    "\n",
    "dirty_data.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above .describe() we can see that all the categorical columns have 500 entries. Hence there are no missing entries present. We can also see that there are 500 unique order_ids hence no errors exist in that. There are 492 unique customer_ids which is also okay as a customer may have multiple orders. We can also see that season has 8 unique entries which is wrong as there are 4 known seasons to us. Also we know that there are only 3 warehouses data but in the nearest_warehouse column we can see 6 unique entries. Hence this is also an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Checking the duplicated values in order_id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data[dirty_data.duplicated(['order_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence there are no duplicate order_id. \n",
    "\n",
    "### 3.2 Now checking the duplicate values in customer_id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data[dirty_data.duplicated(['customer_id'],keep=False)].sort_values(by='customer_id', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output we can see that there are duplicate records for the customers. This is valid as a customer can order multple times from the company. However we find that in the latitude and longitude columns there are wrong values in some rows. This will be corrected later.   \n",
    "\n",
    "### 3.3 Checking for date column's format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating another dataframe which contains three separate columns of year,month,Date\n",
    "coldate = dirty_data['date'].apply(lambda s: pd.Series({'year': s.split('-')[0],\n",
    "                            'month':s.split('-')[1], 'Date':s.split('-')[2]}))\n",
    "coldate = coldate.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the month entries which are greater than 12\n",
    "coldate[(coldate.month > 12)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence there are months that are greater than 12. This is invalid as a month entry cannot be greater than 12. \n",
    "\n",
    "Now I'll check if there are any rows with month and date value both greater than 12. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldate[(coldate.month > 12) & (coldate.Date > 12)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence there are no months and Date columns that are greater than 12. If we would have received a row where both of them would have been gretaer than 12 then we wouldn't be able to handle it. \n",
    "\n",
    "Example, If a row was like 2019-14-17, then we wouldn't know the month of the date at all. Hence we would have to handle it differently. \n",
    "\n",
    "For us however we do not have such scenario where both month and Date columns have value greater than 12. Hence we can simply check for values in month and Date columns. Where I believe month contains values of Dates and Dates contain values of month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_error_a = coldate[(coldate.month > 12) & (coldate.Date <= 12)]\n",
    "new_error_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these rows we can simply interchange the month value and Date value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interchanging or swapping the month and date values\n",
    "coldate[['month','Date']] = coldate[['Date','month']].where((coldate.month > 12) & (coldate.Date <= 12), coldate[['month','Date']].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldate[(coldate.month > 12) & (coldate.Date <= 12)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence now there are no values where month is greater than 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking again for inccorect entries\n",
    "coldate[(coldate.month < 12) & (coldate.Date < 12)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence for such a scenario we assume that the month is correctly placed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldate = coldate.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding errors where year column has values of length less than 4 or date or month column have values with length greater than 2\n",
    "new_error_b = coldate[(coldate.year.str.len() < 4) | (coldate.month.str.len() > 2) | (coldate.Date.str.len() > 2)]\n",
    "new_error_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the desired format is yyyy-mm-dd, hence for the above we can swap the values where Date column has years and year column has dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#swapping date and year values\n",
    "coldate[['year','Date']] = coldate[['Date','year']].where((coldate.year.str.len() < 4) | (coldate.month.str.len() > 2) | (coldate.Date.str.len() > 2), coldate[['year','Date']].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking again for inccorect entries\n",
    "coldate[(coldate.year.str.len() < 4) | (coldate.month.str.len() > 2) | (coldate.Date.str.len() > 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now no incorrect dates exists in the data. Hence we can combine and write back the dates back to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the corrected dates back to the column\n",
    "dirty_data['date'] = coldate['year'].astype(str) + '-' + coldate['month'].astype(str) + '-' + coldate['Date'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence now all date values are correct in the original data frame dirty_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Now checking the nearest warehouse column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.nearest_warehouse.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we see above that there are - \n",
    "\n",
    "6 entries of thompson and 193 entries of Thompson. \n",
    "\n",
    "10 entries of nickolson and 184 entires of Nickolson.\n",
    "\n",
    "6 entries of bakers and 101 entries of Bakers\n",
    "\n",
    "Hence we need to correct this error as thompson and Thompson means the same. Similarly for nickolson and Nickolson and similarly for bakers and Bakers. \n",
    "\n",
    "We can do this by using the replce option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_error_c = dirty_data[dirty_data.nearest_warehouse.isin(['bakers','nickolson','thompson'])]\n",
    "new_error_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing incorrect entries with correct ones\n",
    "dirty_data.nearest_warehouse.replace({\"bakers\": \"Bakers\", \"nickolson\": \"Nickolson\", \"thompson\":\"Thompson\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the unique values again\n",
    "dirty_data.nearest_warehouse.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence now the values are corrected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Checking for the item names in the shopping cart column. There must be 10 unique items as given to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_list = []\n",
    "for i in dirty_data.shopping_cart:\n",
    "    for j in ast.literal_eval(i):\n",
    "        item_list = item_list + [j[0]]\n",
    "        \n",
    "item_set = set(item_list)\n",
    "print(item_set)\n",
    "print(len(item_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence there are 10 unique items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Now to check for the column order_price, we can check for negative values. To do this we can do the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data[dirty_data['order_price'] < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Now analysing the column delivery_charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data[dirty_data['delivery_charges'] < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Analysing longitude and latitude columns, we know that a latitude should be negative and longitude should be positive. (in this case) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data[['customer_lat','customer_long']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the case when latitude > longitude\n",
    "dirty_data[['customer_lat','customer_long']].where(dirty_data.customer_lat > dirty_data.customer_long).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence there are 27 wrong entries in the data. To correct this we can swap the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_error_d = dirty_data[(dirty_data.customer_lat > dirty_data.customer_long)]\n",
    "new_error_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#swapping the incorrect lat,long values\n",
    "dirty_data[['customer_lat','customer_long']] = dirty_data[['customer_long','customer_lat']].where((dirty_data.customer_lat > dirty_data.customer_long), dirty_data[['customer_lat','customer_long']].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking again for the error\n",
    "dirty_data[['customer_lat','customer_long']].where(dirty_data.customer_lat > dirty_data.customer_long).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence now the values are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Checking for negative values in coupon discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data[dirty_data['coupon_discount'] < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence no negative coupon discounts are there. Everything is correct.\n",
    "\n",
    "### 3.10 Checking for negative values in order_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data[dirty_data['order_total'] < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence no negative order_total are there. Everything is correct.\n",
    "\n",
    "### 3.11 Checking for Season values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.season.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence there are incorrect values of winter,summer,autumn,spring. I will convert these into Winter,Summer,Autumn,Spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_error_e = dirty_data[dirty_data.season.isin(['winter','autumn','summer','spring'])]\n",
    "new_error_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing incorrect values with correct ones in season column\n",
    "dirty_data.season.replace({\"winter\": \"Winter\", \"autumn\": \"Autumn\", \"summer\":\"Summer\", \"spring\":\"Spring\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.season.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the values are correct.\n",
    "\n",
    "### 3.12 Checking for is_expedited_delivery values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.is_expedited_delivery.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence only True and False values exists. This is correct \n",
    "\n",
    "### 3.13 Checking for distance_to_nearest_warehouse values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#findng negative values in the column\n",
    "dirty_data[dirty_data['distance_to_nearest_warehouse'] < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence no negative values exist in the column. \n",
    "\n",
    "### 3.13 Checking for latest_customer_review NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data['latest_customer_review'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence no null values exist. \n",
    "\n",
    "### 3.14 Checking for is_happy_customer NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data['is_happy_customer'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence no null values exist. \n",
    "\n",
    "### 3.15 Checking for is_happy_customer NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.is_happy_customer.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence no null values exist. \n",
    "\n",
    "### 3.16 Now to check the sentiments i.e. happy customer or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#object of SentimentIntensityAnalyzer\n",
    "sentiment_analyzer = va.SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a separate dataframe that contains the polarity score as analyzed by the SentimentIntensityAnalyzer\n",
    "is_happy_data = dirty_data['latest_customer_review'].apply(lambda x: pd.Series({'new_sentiment_score' : sentiment_analyzer.polarity_scores(x)['compound']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(is_happy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_happy_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the sentiment score is >= 0.05 then customer is happy else she/he is sad\n",
    "is_happy_data['new_ishappy_customer'] = np.where(is_happy_data['new_sentiment_score'] >= 0.05 , True , False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_happy_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing the actual is_happy_customer column with the analyzed one \n",
    "pd.crosstab(is_happy_data[\"new_ishappy_customer\"], dirty_data[\"is_happy_customer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence there are \n",
    "\n",
    "130 instances where dirty_data file had False as happy_customer value and the customer was not happy. Hence correct data.\n",
    "\n",
    "342 instances where dirty_data file had True as happy_customer value and the customer was happy. Hence correct data.\n",
    "\n",
    "6 instances where dirty_data file had True as happy_customer value but the customer was not happy. Hence incorrect data.\n",
    "\n",
    "22 instances where dirty_data file had False as happy_customer value but the customer was actually happy. Hence incorrect data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_error_i = is_happy_data[is_happy_data[\"new_ishappy_customer\"] != dirty_data[\"is_happy_customer\"]]\n",
    "new_error_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence the correct values determined by the sentiment intensity solver are used in the dirty_data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the analyzed sentiments by SentimentIntensityAnalyzer as the correct column for the dataframe\n",
    "dirty_data['is_happy_customer'] = is_happy_data['new_ishappy_customer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing again using crosstab\n",
    "pd.crosstab(is_happy_data[\"new_ishappy_customer\"], dirty_data[\"is_happy_customer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.17 Checking date and seasons whether the seasons are correct based on the date\n",
    "\n",
    "We know, \n",
    "\n",
    "Spring - September,October,November\n",
    "\n",
    "Autumn - March,April,May\n",
    "\n",
    "Summer - December,January,February \n",
    "\n",
    "Winter - June,July,August\n",
    "\n",
    "Hence we can check whether the season column in dirty_data is correct or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the dates to the desired format\n",
    "dirty_data['date'] = pd.to_datetime(dirty_data['date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding only the months from the date to compare with seasons later on\n",
    "dirty_data['month_only'] = dirty_data['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirty_data['new_season'] = np.where(dirty_data['month_only'] >= 0.05 , 'True' , 'False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new column new_season to calculate correct seasons\n",
    "dirty_data['new_season'] = np.where((dirty_data.month_only < 6) & (dirty_data.month_only > 2), 'Autumn',\n",
    "                           np.where((dirty_data.month_only < 9) & (dirty_data.month_only > 5), 'Winter',\n",
    "                           np.where((dirty_data.month_only < 12) & (dirty_data.month_only > 8), 'Spring','Summer')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing new_season and original one\n",
    "pd.crosstab(dirty_data[\"season\"], dirty_data[\"new_season\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, there is some incorrect data.\n",
    "\n",
    "1 instance where dirty_data had season as Autumn but the actual season was Spring\n",
    "\n",
    "2 instances where dirty_data had season as Autumn but the actual season was Summer\n",
    "\n",
    "2 instances where dirty_data had season as Autumn but the actual season was Winter\n",
    "\n",
    "2 instances where dirty_data had season as Spring but the actual season was Autumn\n",
    "\n",
    "1 instance where dirty_data had season as Spring but the actual season was Summer\n",
    "\n",
    "1 instance where dirty_data had season as Summer but the actual season was Spring\n",
    "\n",
    "3 instances where dirty_data had season as Winter but the actual season was Autumn\n",
    "\n",
    "1 instance where dirty_data had season as Winter but the actual season was Spring\n",
    "\n",
    "3 instances where dirty_data had season as Winter but the actual season was Summer\n",
    "\n",
    "\n",
    "\n",
    "Hence, we can use the correct season column i.e. new_season as the correct season column and drop the original season column as it contains incorrect values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_error_f = dirty_data[dirty_data.season != dirty_data.new_season]\n",
    "new_error_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the season column original one and using the new calculated one\n",
    "dirty_data.drop('season',axis = 1 ,inplace = True)\n",
    "dirty_data.drop('month_only', axis = 1,inplace=True)\n",
    "dirty_data.rename(columns={'new_season': 'season'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.18 Now finding whether the nearest warehouse column is correct or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse_data = pd.read_csv(\"warehouses.csv\")\n",
    "warehouse_data.set_index('names', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference from www.stackoverflow.com\n",
    "#function to find distance between two locations and respective nearest warehouse name\n",
    "def find_nearest_warehouse(lat1,lon1):\n",
    "\n",
    "    # given radius of earth in km\n",
    "    R = 6378.0\n",
    "\n",
    "    #converting lat long values to radian\n",
    "    lat1 = radians(lat1)\n",
    "    lon1 = radians(lon1)\n",
    "    #converting lat long values of three warehouses to radian\n",
    "    nick_lat = radians(warehouse_data.loc['Nickolson','lat'])\n",
    "    nick_lon = radians(warehouse_data.loc['Nickolson','lon'])\n",
    "    thomp_lat = radians(warehouse_data.loc['Thompson','lat'])\n",
    "    thomp_lon = radians(warehouse_data.loc['Thompson','lon'])\n",
    "    baker_lat = radians(warehouse_data.loc['Bakers','lat'])\n",
    "    baker_lon = radians(warehouse_data.loc['Bakers','lon'])\n",
    "\n",
    "    dlon_nic = nick_lon - lon1\n",
    "    dlat_nic = nick_lat - lat1\n",
    "\n",
    "    a = sin(dlat_nic / 2)**2 + cos(lat1) * cos(nick_lat) * sin(dlon_nic / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    #variable to store distance from given point to nickolson warehouse\n",
    "    distance_nic = R * c\n",
    "    \n",
    "    \n",
    "    dlon_thomp = thomp_lon - lon1\n",
    "    dlat_thomp = thomp_lat - lat1\n",
    "\n",
    "    a = sin(dlat_thomp / 2)**2 + cos(lat1) * cos(thomp_lat) * sin(dlon_thomp / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    #variable to store distance from given point to thompson warehouse\n",
    "    distance_thomp = R * c\n",
    "    \n",
    "    \n",
    "    dlon_baker = baker_lon - lon1\n",
    "    dlat_baker = baker_lat - lat1\n",
    "\n",
    "    a = sin(dlat_baker / 2)**2 + cos(lat1) * cos(baker_lat) * sin(dlon_baker / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    #variable to store distance from given point to bakers warehouse\n",
    "    distance_baker = R * c\n",
    "    \n",
    "    warehouse_dict = {'Nickolson':distance_nic,'Thompson':distance_thomp,'Bakers':distance_baker}\n",
    "    warehouse_name = min(warehouse_dict, key = lambda k : warehouse_dict[k])\n",
    "    distance_value = min(warehouse_dict.values())\n",
    "    #returning the nearest warehouse name and distance\n",
    "    return warehouse_name,distance_value\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the new warehouse distance and new nearest warehouse values\n",
    "dirty_data['new_nearest_warehouse'],dirty_data['new_distance_warehouse'] = zip(*dirty_data.apply(lambda x: find_nearest_warehouse(x['customer_lat'], x['customer_long']), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dirty_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly looking at Nearest Warehouse original and new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(dirty_data[\"nearest_warehouse\"], dirty_data[\"new_nearest_warehouse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, there is some incorrect data.\n",
    "\n",
    "2 instances where dirty_data had nearest_warehouse as Bakers but the actual nearest_warehouse was Nickolson\n",
    "\n",
    "6 instances where dirty_data had nearest_warehouse as Bakers but the actual nearest_warehouse was Thompson\n",
    "\n",
    "2 instances where dirty_data had nearest_warehouse as Nickolson but the actual nearest_warehouse was Bakers\n",
    "\n",
    "6 instances where dirty_data had nearest_warehouse as Nickolson but the actual nearest_warehouse was Thompson\n",
    "\n",
    "2 instances where dirty_data had nearest_warehouse as Thompson but the actual nearest_warehouse was Bakers\n",
    "\n",
    "2 instances where dirty_data had nearest_warehouse as Thompson but the actual nearest_warehouse was Nickolson\n",
    "\n",
    "\n",
    "Hence, we can use the correct nearest_warehouse column i.e. new_nearest_warehouse as the correct column and drop the original nearest_warehouse column as it contains incorrect values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_error_g = dirty_data[dirty_data['nearest_warehouse'] != dirty_data['new_nearest_warehouse']]\n",
    "new_error_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the original column and using the new calculated one\n",
    "dirty_data.drop('nearest_warehouse',axis = 1 ,inplace = True)\n",
    "dirty_data.rename(columns={'new_nearest_warehouse': 'nearest_warehouse'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now looking at original and new nearest_distance_warehouse. We can observe that the new_distance_warehouse has numbers to larger decimal places. Hence for correct comparison we can round this column to 4 places as in the same format of distance_to_nearest_warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data['new_distance_warehouse'] = dirty_data['new_distance_warehouse'].round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_distance_nearest = dirty_data['distance_to_nearest_warehouse'] == dirty_data['new_distance_warehouse']\n",
    "\n",
    "check_distance_nearest.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence there are 33 values that are incorrect in the column distance_to_nearest_warehouse. \n",
    "\n",
    "To get the correct data we can simply use the new column new_distance_warehouse as it contains the correct values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_error_h = dirty_data[dirty_data['distance_to_nearest_warehouse'] != dirty_data['new_distance_warehouse']]\n",
    "new_error_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the original column and using the new calculated one\n",
    "\n",
    "dirty_data.drop('distance_to_nearest_warehouse',axis = 1 ,inplace = True)\n",
    "dirty_data.rename(columns={'new_distance_warehouse': 'distance_to_nearest_warehouse'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.19 ORDER PRICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the order price by findng the price of each item in the cart. This is done by using the shopping cart column and the quantity column. \n",
    "\n",
    "Firstly I have created a function find_df which will create columns that will include the quantity , items only and the length and frequency of each item tuple. This will be used in linear algebra to solve for the price of each individual product. \n",
    "\n",
    "Next I have ceated a function that is used to simply find the solution of the equatons created by np.array(). The linear algebra multiplies the matrices where first matrix is the quantities of the product and the second matrix is the given price of the cart. Using this the price of each item can be calculated. Using this I have reated a dicttionary finally that will store the product as the key and its price as the value\n",
    "\n",
    "In the next function  I have used the calculated prices and the carts to get the new column new_order_price. This will be used to find the correct order price and hence can be used to compare the given dirty data and the calculated data for order price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "def find_df(check_df):\n",
    "    df = check_df.copy()\n",
    "    #df = check_df.filter(['shopping_cart'])\n",
    "    \n",
    "    #converrting the string literals of shopping cart to lists\n",
    "    df['shopping_cart'] = df['shopping_cart'].apply(lambda x: literal_eval(str(x)))\n",
    "    df['shopping_cart'] = df['shopping_cart'].apply(sorted)\n",
    "    #df['order_price'] = check_df.filter(['order_price'])\n",
    "\n",
    "    #list will store dictionaries, where key will be item and value will be quantity\n",
    "    new_list = []\n",
    "    for i in df.shopping_cart:\n",
    "        new_dict = {}\n",
    "        for j in i:\n",
    "            new_dict[j[0]] = j[1]\n",
    "        new_list.append(new_dict)\n",
    "\n",
    "\n",
    "    #column to store just the items \n",
    "    df['items_only'] = ''\n",
    "    \n",
    "    #column to store just the quantities\n",
    "    df['quantity_only'] = ''\n",
    "    \n",
    "    #column to store just the length of the cart\n",
    "    df['length'] = ''\n",
    "\n",
    "    for index,i in enumerate(new_list):\n",
    "\n",
    "        temp_item_list = []\n",
    "        temp_quantity_list = []\n",
    "        for j,k in i.items():\n",
    "            temp_item_list.append(j)\n",
    "            temp_quantity_list.append(k)\n",
    "\n",
    "        df.at[index,'items_only'] = temp_item_list\n",
    "        df.at[index,'quantity_only'] = temp_quantity_list\n",
    "        df.at[index,'length'] = len(temp_item_list)\n",
    "\n",
    "\n",
    "    df['items_only'] = df['items_only'].apply(lambda x :tuple(x))\n",
    "\n",
    "    #column to store frequency of the repeated items in the tuple\n",
    "    df['occurence'] = df['items_only'].apply(lambda x: (df['items_only'] == x).sum())\n",
    "\n",
    "    return df\n",
    "    \n",
    "    \n",
    "\n",
    "def find_price(df):\n",
    "    \n",
    "    #selecting only those rows where length is equal to occurence as it will be used to solve the linear algebra\n",
    "    df = df[df.length == df.occurence]\n",
    "\n",
    "    occurence_val = df.occurence.value_counts().index.tolist()\n",
    "    occurence_occurence = df.occurence.value_counts().tolist()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    f_dic = {}\n",
    "    for i in occurence_val:\n",
    "        find_item = []\n",
    "        new_df = df[df.occurence == i]\n",
    "        find_item = new_df.items_only.value_counts().index.to_list()\n",
    "        for j in find_item:\n",
    "            try:           \n",
    "                #coefficients of quantities\n",
    "                cal_a = np.array(list(new_df.quantity_only[new_df.items_only == j]))\n",
    "                \n",
    "                #coefficients of price\n",
    "                cal_b = np.array(list(new_df.order_price[new_df.items_only == j]))\n",
    "                cal_price = np.linalg.solve(cal_a,cal_b)\n",
    "                \n",
    "                #dctionary to store the item name and price\n",
    "                f_dic.update(dict(zip(new_df['items_only'][new_df['items_only']==j].values[0],cal_price)))\n",
    "           \n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    return f_dic\n",
    "\n",
    "\n",
    "\n",
    "def fill_new_column(df,f_dic):\n",
    "     \n",
    "    for i in df.index:\n",
    "        item_tuple = df.at[i,'items_only']\n",
    "        quant_list = df.at[i,'quantity_only']\n",
    "        for k in item_tuple:\n",
    "            try:\n",
    "                #filling the calculated order price new one\n",
    "                df.at[i,'new_order_price'] = df.at[i,'new_order_price'] + f_dic[k]*quant_list[item_tuple.index(k)]\n",
    "            except:\n",
    "                continue\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have used the outlier_dataset to calculate the price of the items as it will not have any incorect order_price values. Hence I have used it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the outlier dataset to calculate the price of items\n",
    "out_find_price = find_df(outlier_data)\n",
    "dic_price_final = find_price(out_find_price)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_find_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_find_price = find_df(dirty_data)\n",
    "data_find_price['new_order_price'] = 0 \n",
    "data_find_price = fill_new_column(data_find_price,dic_price_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_find_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_find_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_find_price['order_price'][data_find_price['order_price'] != data_find_price['new_order_price']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence 54 times we can say that the order price calculated is wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To correct this error, we can use the new_order_price column and drop the original one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the original column and using the new calculated one\n",
    "\n",
    "data_find_price.drop('order_price',axis = 1 ,inplace = True)\n",
    "data_find_price.rename(columns={'new_order_price': 'order_price'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_find_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.20 Checking for order total\n",
    "\n",
    "\n",
    "\n",
    "Now we can check whether the calculation of order_price, discount and delivery charge was done correctly or not. \n",
    "\n",
    "To do so I have used the concept - \n",
    "\n",
    "Order Total = Order Price - Coupon Discount(in %) + Delivery Charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_find_price[['order_price','coupon_discount','delivery_charges','order_total']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new order_total column with correct values\n",
    "data_find_price['new_order_total'] = data_find_price['order_price'] - ((data_find_price['coupon_discount']*data_find_price['order_price'])/100) + data_find_price['delivery_charges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_find_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_find_price[data_find_price['order_total'] != data_find_price['new_order_total']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence there are wrongly calculated values in the order_total. The correct values are present in the new_order_total column which have been calculated above. \n",
    "\n",
    "Now simply, we can use the new_order_total column as the order_total column and can drop the original order_total column with wrong values. \n",
    "\n",
    "Hence the values have been corrected now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_find_price.drop('order_total',axis = 1 ,inplace = True)\n",
    "data_find_price.rename(columns={'new_order_total': 'order_total'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_find_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_find_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.19 changing the order of the columns as they were originally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_find_price = data_find_price[cols]\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.20 changing the data types of the columns as they were originally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_find_price.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_find_price.date = data_find_price.date.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_find_price.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the data to output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_find_price.to_csv('30759307_dirty_data_solution.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MISSING DATA ANALYSIS\n",
    "\n",
    "In this task we are required to find the missing data and impute the missing values based on other related columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading missing data file\n",
    "missing_data = pd.read_csv(\"30759307_missing_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#information about the dataframe\n",
    "missing_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols2 = missing_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the count of missing values in each column\n",
    "missing_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 The missing nearest_warehouse and the distance_to_nearest_warehouse can be imputed by using the latitude and longitude values of the customer and the location of the warehouses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the null distance values\n",
    "missing_data[missing_data['distance_to_nearest_warehouse'].isnull()].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the null warehouse name values\n",
    "missing_warehou = missing_data[missing_data['nearest_warehouse'].isnull()]\n",
    "\n",
    "missing_data = missing_data.drop(missing_warehou.index)\n",
    "\n",
    "#creating two new columns for the missing data about warehouse and distance to nearest warehouse\n",
    "missing_warehou['new_nearest_warehouse'],missing_warehou['new_distance_warehouse']= zip(*missing_warehou.apply(lambda x: find_nearest_warehouse(x['customer_lat'], x['customer_long']), axis=1))\n",
    "\n",
    "\n",
    "#deleting the incorrect column\n",
    "del missing_warehou['nearest_warehouse']\n",
    "\n",
    "del missing_warehou['distance_to_nearest_warehouse']\n",
    "\n",
    "missing_warehou.rename({'new_nearest_warehouse':'nearest_warehouse','new_distance_warehouse':'distance_to_nearest_warehouse'},axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merging the two dataframes back together\n",
    "missing_data = pd.concat([missing_data,missing_warehou],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence now checking for the nearest warehouse and the distance to nearest warehouse, we can check null values in the new generated columns above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data.distance_to_nearest_warehouse = missing_data.distance_to_nearest_warehouse.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data[missing_data['nearest_warehouse'].isnull()].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data[missing_data['distance_to_nearest_warehouse'].isnull()].head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence no null values exist in these columns. \n",
    "\n",
    "Hence we can use these new columns as the correct columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again checking the sum of rows with mssing values in the dataframe\n",
    "missing_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Now to calculate the missing order_prices from the functions used in Dirty Data Analysis part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to calculate the missing values of the order_prices we can use the functions created in Dirty Data Analysis task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_miss_price = find_df(missing_data)\n",
    "data_miss_price['new_order_price'] = 0 \n",
    "missing_data = fill_new_column(data_miss_price,dic_price_final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again checking the sum of rows with mssing values in the dataframe\n",
    "\n",
    "missing_data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "missing_data.drop('order_price',axis = 1 ,inplace = True)\n",
    "missing_data.rename(columns={'new_order_price': 'order_price'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence the new_order_price contains no null values. So we can simply use that as the new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again checking the sum of rows with mssing values in the dataframe\n",
    "\n",
    "missing_data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 To find the missing delivery charges we can use the equation below- \n",
    "\n",
    "order_total = order_price - ((coupon_discount*order_price)/100) + delivery_charges\n",
    "\n",
    "Hence,\n",
    "\n",
    "delivery_charges = order_total - order_price + ((coupon_discount*order_price)/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #filling the column with the correct delivery charges\n",
    "missing_data['delivery_charges'] = missing_data['order_total'] - missing_data['order_price'] + ((missing_data['coupon_discount']*missing_data['order_price'])/100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again checking the sum of rows with mssing values in the dataframe\n",
    "\n",
    "missing_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Now to calculate the missing values of the order total, we can use the updated order prices, coupon discount and delivery charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_order = missing_data[missing_data['order_total'].isnull()].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new order_total column with correct values\n",
    "miss_order['new_order_total'] = miss_order['order_price'] - ((miss_order['coupon_discount']*miss_order['order_price'])/100) + miss_order['delivery_charges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = missing_data.drop(miss_order.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the original incorrect column and using the new calculated column values\n",
    "miss_order.drop('order_total',axis = 1 ,inplace = True)\n",
    "miss_order.rename(columns={'new_order_total': 'order_total'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = pd.concat([missing_data,miss_order],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again checking the sum of rows with mssing values in the dataframe\n",
    "\n",
    "missing_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Now to find the missing values of is_happy_customer column, we can use the Sentiment Intensity Analysis approach as used above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding null is_happy_customer values\n",
    "missing_data[missing_data['is_happy_customer'].isnull()].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the correct sentiments of the customer based on the reviews\n",
    "find_happy_data = missing_data['latest_customer_review'].apply(lambda x: pd.Series({'new_sentiment_score' : sentiment_analyzer.polarity_scores(x)['compound']}))\n",
    "\n",
    "#customer is happy so True is polarity score >= 0.5 else False\n",
    "find_happy_data['new_ishappy_customer'] = np.where(find_happy_data['new_sentiment_score'] >= 0.05 , 1.0 , 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using this new is_happy_customer as the final column\n",
    "missing_data['is_happy_customer'] = find_happy_data['new_ishappy_customer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data[missing_data['is_happy_customer'].isnull()].head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence the column has been imputed with the correct values based on the reviews of the customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again checking the sum of rows with mssing values in the dataframe\n",
    "\n",
    "missing_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Changing the order of the columns as they were originally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = missing_data[cols2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the file back to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data.to_csv('30759307_missing_data_solution.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. OUTLIER DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_data = pd.read_csv(\"30759307_outlier_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_data.boxplot('delivery_charges', figsize=(10, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However it is not good to just determine the outliers based on its values. We can identify outliers based on the columns that it depends upon. These are season, distance_to_nearest_warehouse, whether the customer wants an expedited delivery and whether the customer was happy or not with previous order.\n",
    "\n",
    "To find the relationship with these, we can draw a boxplot based on these column values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_data.boxplot('delivery_charges', by = ['season'] , figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we can see that the Winter column has outliers above approximately 100. So if we remove all outliers in the data above 100 then we would lose a large portion of the data for the seasons spring and summer. Hence it is not a good approach to simply remove the outliers based on season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_data.boxplot('delivery_charges', by = ['is_expedited_delivery'] , figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again by just observing and removing outliers from the column is_expedited_delivery is not a good approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_data.boxplot('delivery_charges', by = ['is_happy_customer'] , figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_data.boxplot('delivery_charges', by = ['season','is_happy_customer'] , figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_data.boxplot('delivery_charges', by = ['season','is_happy_customer','is_expedited_delivery'] , figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we can create separate dataframes based on the season, and then based on the values of the is_expedited_delivery column and is_happy_customer column, we can remove the outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summer_outlier = outlier_data[outlier_data.season == 'Summer']\n",
    "winter_outlier = outlier_data[outlier_data.season == 'Winter']\n",
    "autumn_outlier = outlier_data[outlier_data.season == 'Autumn']\n",
    "spring_outlier = outlier_data[outlier_data.season == 'Spring']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summer_outlier.boxplot('delivery_charges', by = ['is_expedited_delivery','is_happy_customer'] , figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence in this it will be easier to remove the outliers as we can use the combinations of True and False for the two columns is_expedited_delivery and is_happy_customer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winter_outlier.boxplot('delivery_charges', by = ['is_expedited_delivery','is_happy_customer'] , figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence in this it will be easier to remove the outliers as we can use the combinations of True and False for the two columns is_expedited_delivery and is_happy_customer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autumn_outlier.boxplot('delivery_charges', by = ['is_expedited_delivery','is_happy_customer'] , figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spring_outlier.boxplot('delivery_charges', by = ['is_expedited_delivery','is_happy_customer'] , figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence in this it will be easier to remove the outliers as we can use the combinations of True and False for the two columns is_expedited_delivery and is_happy_customer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summer_outlier.shape)\n",
    "print(winter_outlier.shape)\n",
    "print(autumn_outlier.shape)\n",
    "print(spring_outlier.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence this is the shape of the dataframes before removing outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reframe_df(df):\n",
    "    #getting True and False combinations of the two columns\n",
    "    \n",
    "    df_true_true = df[(df.is_expedited_delivery == True) & (df.is_happy_customer == True)]\n",
    "    df_true_false = df[(df.is_expedited_delivery == True) & (df.is_happy_customer == False)]\n",
    "    df_false_true = df[(df.is_expedited_delivery == False) & (df.is_happy_customer == True)]\n",
    "    df_false_false = df[(df.is_expedited_delivery == False) & (df.is_happy_customer == False)]\n",
    "    \n",
    "\n",
    "    #calculating the q1 value\n",
    "    q1 = np.quantile(df_true_true['delivery_charges'], .25)\n",
    "    #calculating the q3 value\n",
    "    q3 = np.quantile(df_true_true['delivery_charges'], .75)\n",
    "    #calculating the iqr value\n",
    "    iqr = q3-q1\n",
    "    upper_range = q3+1.5*iqr\n",
    "    lower_range = q1-1.5*iqr\n",
    "    #removing outliers below and above q3*1.5iqr\n",
    "    df_true_true = df_true_true[(df_true_true.delivery_charges <= upper_range) & (df_true_true.delivery_charges >= lower_range)]\n",
    "\n",
    "    \n",
    "    #calculating the q1 value   \n",
    "    q1 = np.quantile(df_true_false['delivery_charges'], .25)\n",
    "    #calculating the q3 value\n",
    "    q3 = np.quantile(df_true_false['delivery_charges'], .75)\n",
    "    #calculating the iqr value\n",
    "    iqr = q3-q1\n",
    "    upper_range = q3+1.5*iqr\n",
    "    lower_range = q1-1.5*iqr\n",
    "    df_true_false = df_true_false[(df_true_false.delivery_charges <= upper_range) & (df_true_false.delivery_charges >= lower_range)]\n",
    "    \n",
    "    #calculating the q1 value\n",
    "    \n",
    "    q1 = np.quantile(df_false_true['delivery_charges'], .25)\n",
    "    #calculating the q3 value\n",
    "    q3 = np.quantile(df_false_true['delivery_charges'], .75)\n",
    "    #calculating the iqr value\n",
    "    iqr = q3-q1\n",
    "    upper_range = q3+1.5*iqr\n",
    "    lower_range = q1-1.5*iqr\n",
    "    df_false_true = df_false_true[(df_false_true.delivery_charges <= upper_range) & (df_false_true.delivery_charges >= lower_range)]\n",
    "\n",
    "    \n",
    "    #calculating the q1 value\n",
    "    \n",
    "    q1 = np.quantile(df_false_false['delivery_charges'], .25)\n",
    "    #calculating the q3 value\n",
    "    q3 = np.quantile(df_false_false['delivery_charges'], .75)\n",
    "    #calculating the iqr value\n",
    "    iqr = q3-q1\n",
    "    upper_range = q3+1.5*iqr\n",
    "    lower_range = q1-1.5*iqr\n",
    "    df_false_false = df_false_false[(df_false_false.delivery_charges <= upper_range) & (df_false_false.delivery_charges >= lower_range)]\n",
    "\n",
    "    #merginf the datasets back together\n",
    "    merged_df = pd.concat([df_true_true, df_true_false,df_false_true,df_false_false])\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summer_outlier = reframe_df(summer_outlier)\n",
    "winter_outlier = reframe_df(winter_outlier)\n",
    "autumn_outlier = reframe_df(autumn_outlier)\n",
    "spring_outlier = reframe_df(spring_outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diaplaying the shape of the datasets again after removing outliers\n",
    "print(summer_outlier.shape)\n",
    "print(winter_outlier.shape)\n",
    "print(autumn_outlier.shape)\n",
    "print(spring_outlier.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can merge all of them. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_outlier = pd.concat([summer_outlier, winter_outlier,spring_outlier,autumn_outlier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_outlier.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_outlier.to_csv('30759307_outlier_data_solution.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hencce the outliers have been removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this assignment we have used and learned a lot about wrangng and data cleaning. The following take aways from this for me are - \n",
    "\n",
    "Sentiment Analysis - It was interesting to determine how from a sentecne we can determine the sentiments of an individual. By directly using the libraries and functions in python we can analyse and interpret how a person is feeling by measuring the polarity scores derieved from the text. \n",
    "\n",
    "Data Cleansing - I learned how to handle different data types and how to format that in the required condition. I learned how to handle and check the format of date column and also learned how to deal with bool type columns. \n",
    "\n",
    "Geographical Distance - I have also learned how to calculate distance between two points given their latitutde and longitude values. It was interesting to find out the nearest warehouses based on the values given to us\n",
    "\n",
    "Graphical Representatons - I have analysed and learnt about the boxplot and the inter quartile ranges. I also learned how to fix the outliers\n",
    "\n",
    "Dealing wth missing data - I have learned how to  deal with missing values and how to impute them from other related column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "www.stackoverflow.com\n",
    "\n",
    "www.w3schools.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
